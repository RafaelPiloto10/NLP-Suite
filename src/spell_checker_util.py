import sys
import GUI_util
import IO_libraries_util

#'spellchecker','pyspellchecker', # this gives problems; disconnected from checking the package

if not IO_libraries_util.install_all_packages(GUI_util.window,"spell_checker_util",['nltk','tkinter','os','autocorrect','stanfordcorenlp','pandas','collections']):
    sys.exit(0)

import os
from nltk.stem import WordNetLemmatizer

from nltk import tokenize
import nltk
IO_libraries_util.import_nltk_resource(GUI_util.window,'tokenizers/punkt','punkt')

import pandas as pd
from stanfordcorenlp import StanfordCoreNLP
import collections
import tkinter.messagebox as mb
from autocorrect import spell
from spellchecker import SpellChecker
from textblob import Word
from pandas import DataFrame


import Excel_util
import IO_csv_util
import IO_files_util
import IO_user_interface_util

def lemmatizing(word):#edited by Claude Hu 08/2020
    #https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python
    pos = ['n', 'v','a', 's', 'r']#list of postags
    result = word
    for p in pos:
        # if lemmatization with any postag gives different result from the word itself
        # that lemmatization is returned as result
        lemmatizer = WordNetLemmatizer()
        lemma = lemmatizer.lemmatize(word, p)
        if lemma != word:
            result = lemma
            break
    return result

# https://www.nltk.org/book/ch02.html
def nltk_unusual_words(window,inputFilename,inputDir,output_dir_path, openOutputFiles, createExcelCharts=True):
    filesToOpen=[]
    unusual=[]
    container=[]
    documentID=0
    files=IO_files_util.getFileList(inputFilename, inputDir, '.txt')
    nFile=len(files)
    if nFile==0:
        return
    if nFile>1:
        Dir=os.path.basename(os.path.normpath(inputDir))
        outputFilename=IO_files_util.generate_output_file_name('Dir_' + Dir, output_dir_path, '.csv', 'NLTK_unus', 'stats')
    else:
        outputFilename=IO_files_util.generate_output_file_name(inputFilename, output_dir_path, '.csv', 'NLTK_unus', 'stats')
    filesToOpen.append(outputFilename)

    # already shown in NLP.py
    # IO_util.timed_alert(GUI_util.window,3000,'Analysis start','Started running NLTK unusual words at',True,'You can follow NLTK unusual words in command line.')
    for file in files:
        documentID=documentID+1
        print("\nProcessing file " + str(documentID) + "/" + str(nFile) + ' ' + file)
        text = (open(file, "r", encoding="utf-8", errors="ignore").read())
        #lemmatizer = WordNetLemmatizer()
        # text_vocab = set(lemmatizer.lemmatize(w.lower()) for w in text.split(" ") if w.isalpha())
        text_vocab = set(lemmatizing(w.lower()) for w in text.split(" ") if w.isalpha())
        english_vocab = set([w.lower() for w in nltk.corpus.words.words()])
        print("english_vocab",english_vocab)
        print("text_vocab",text_vocab)
        unusual = text_vocab - english_vocab
        #convert the set to a list
        unusual=list(unusual)
        #sort the list
        unusual.sort()
        # unusual = [[documentID, file, word] for word in unusual]
        unusual = [[documentID, IO_csv_util.dressFilenameForCSVHyperlink(file), word] for word in unusual]
        container.extend(unusual)
    container.insert(0, ['Document ID', 'Document', 'Misspelled/unusual word'])
    if len(container)>0:
        if IO_csv_util.list_to_csv(window,container,outputFilename):
            outputFilename=''
            return outputFilename
    else:
        IO_user_interface_util.timed_alert(GUI_util.window, 3000, 'Spelling checker (via nltk)', 'No misspelled/unusual words found in\n' + file, True)
        if nFile==1:
            return

    if len(inputDir) != 0:
        mb.showwarning(title='Warning', message='The output filename generated by NLTK spelling checker is the name of the directory processed in input, rather than any individual file in the directory.\n\nThe output csv file includes all ' + str(nFile) + ' files in the input directory processed by NLTK.')

    # NLTK unusual words
    if createExcelCharts:
        # if nFile>10:
        #     result = mb.askyesno("Excel charts","You have " + str(nFile) + " files for which to compute Excel charts.\n\nTHIS WILL TAKE A LONG TIME.\n\nAre you sure you want to do that?")
        #     if result==True:
        columns_to_be_plotted = [[0,2]]
        hover_label=['']

        # Excel_outputFileName = Excel_util.run_all(columns_to_be_plotted, inputFilename, output_dir_path,
        #                                           outputFileLabel='NLTK_spell',
        #                                           chart_type_list=["bar"],
        #                                           chart_title='Misspelled/Unusual word frequency',
        #                                           column_xAxis_label_var='',
        #                                           hover_info_column_list=hover_label,
        #                                           count_var=1)
        # if Excel_outputFileName != "":
        #     filesToOpen.append(Excel_outputFileName)

        # outputFileNameXLSM = Excel_util.run_all(columns_to_be_plotted,inputFilename,output_dir_path,
        # outputFilename, chart_type_list = ["bar"], chart_title= "Misspelled/Unusual word frequency",
        # column_xAxis_label_var = '',column_yAxis_label_var = '',outputExtension = '.xlsm',label1='NLTK',
        # label2='bar',label3='chart',label4='',label5='', useTime=False,disable_suffix=True,  count_var=1, column_yAxis_field_list = [], reverse_column_position_for_series_label=False , series_label_list=[''], second_y_var=0, second_yAxis_label='', hover_info_column_list=hover_label)
        # if outputFileNameXLSM != "":
        #     filesToOpen.append(outputFileNameXLSM)

    if openOutputFiles==True:
        IO_files_util.OpenOutputFiles(GUI_util.window, openOutputFiles, filesToOpen)
        filesToOpen = [] # do not open again in calling function

    # already shown in NLP.py
    # IO_util.timed_alert(GUI_util.window,3000,'Analysis end','Finished running NLTK unusual words at',True)
    for u in unusual:
        print(u[-1])

    print(len(unusual))
    return outputFilename

def generate_simple_csv(Dataframe):
    pass

def createChart(inputFilename,outputDir,columns_to_be_plotted,hover_label):
    Excel_outputFileName = Excel_util.run_all(columns_to_be_plotted, inputFilename, outputDir,
                                              outputFileLabel='Leven_spell',
                                              chart_type_list=["pie"],
                                              chart_title='Frequency of Potential Typos',
                                              column_xAxis_label_var='',
                                              hover_info_column_list=hover_label,
                                              count_var=1)
    return Excel_outputFileName


# check within subdirectory
def check_for_typo_sub_dir(CoreNLPDir, inputDir, outputDir, openOutputFiles, createExcelCharts, NERs, similarity_value, by_all_tokens_var,spelling_checker_var):
    filesToOpen=[]
    subdir = [f.path for f in os.scandir(inputDir) if f.is_dir()]
    if subdir == []:
        mb.showwarning(title='Check Subdir option',
                       message='There are no sub directories under the selected input directory\n\n' + inputDir +'\n\nPlease, uncheck your subdir option if you want to process this directory and try again.')
    df_list = []
    for dir in subdir:
        dfs = check_for_typo(CoreNLPDir, dir, outputDir, openOutputFiles, createExcelCharts, NERs, similarity_value, by_all_tokens_var,spelling_checker_var)
        df_list.append(dfs)
    if len(df_list) > 0:
        df_complete_list = [df[0] for df in df_list]
        df_simple_list = [df[1] for df in df_list]
        df_complete = pd.concat(df_complete_list, ignore_index=True)
        df_simple = pd.concat(df_simple_list, ignore_index=True)
        df_simple.to_csv(outputFileName_simple, index=False)
        df_complete.to_csv(outputFileName_complete, index=False)

        filesToOpen.append(outputFileName_simple)
        filesToOpen.append(outputFileName_complete)

        if createExcelCharts:
            Excel_outputFileName = createChart(outputFileName_simple,outputDir, [[10, 10]], '')
            if Excel_outputFileName!="":
                filesToOpen.append(Excel_outputFileName)

        if openOutputFiles == True:
            IO_files_util.OpenOutputFiles(GUI_util.window, openOutputFiles, filesToOpen)
            filesToOpen=[] # empty the list to avoid opening files twice

    return filesToOpen

# the check for typo function
# check whether a single word is considered as typo within a list of words
# design choice for this algorithm:
#   if the word is shorter than user-supplied word length (default 4 characters):
#       1 or more character mistake will be considered as typo
#   else(the word is longer than or equal to user-supplied word length (default 4 characters):
#       2 or more character mistake will be considered as typo

# checklist contains words with more than 1 time of appearence
# similarity_value is the gaging_difference attribute
def check_edit_dist(input_word, checklist, similarity_value):
    exist_typo = False
    for word in checklist:
        dist = nltk.edit_distance(input_word, word[0])
        if len(input_word) >= similarity_value:
            if 0 < dist <= 2:
                exist_typo = True
                return exist_typo, word[0], word[1]
        else:
            if 0 < dist <= 1:
                exist_typo = True
                return exist_typo, word[0], word[1]
    return exist_typo, '', ''


def spellchecking_autocorrect(text: str) -> (str, DataFrame):
    original_str_list = []
    new_str_list = []
    speller = SpellChecker()
    for word in nltk.word_tokenize(text):
        if word.isalnum():
            original_str_list.append(word)
            new_word = speller.correction(word)
            if new_word != word:
                new_str_list.append(new_word)
            else:
                new_str_list.append('')
    return spell(text), DataFrame({
        'Original': original_str_list,
        'Corrected': new_str_list
    })

def spellchecking_pytesseract(inputDir,outputDir):
    misspelled = spell.unknown([word])
    if misspelled == set():
        return False,''
    else:
        for misspell in misspelled:
            # Get the one `most likely` answer
            return True, spell.correction(misspell)


def spellchecking_pyspellchecker(text: str) -> (str, DataFrame):
    # :: pyspellchecker seems to remove punctuations.
    new_str_list = []
    original_str_list = []
    new_str_list_for_df = []
    treebank = nltk.tokenize.treebank.TreebankWordDetokenizer()
    speller = SpellChecker()
    for word in nltk.word_tokenize(text):
        if word.isalnum():
            original_str_list.append(word)
            new_word = speller.correction(word)
            if new_word != word:
                new_str_list_for_df.append(new_word)
            else:
                new_str_list_for_df.append('')
        new_str_list.append(word)
    return treebank.detokenize(new_str_list), DataFrame({
        'Original': original_str_list,
        'Corrected': new_str_list_for_df
    })


def spellchecking_text_blob(text: str) -> (str, DataFrame):
    new_str_list = []
    new_str_list_for_df = []
    original_str_list = []
    treebank = nltk.tokenize.treebank.TreebankWordDetokenizer()
    for word in nltk.word_tokenize(text):
        if word.isalnum():
            original_str_list.append(word)
            new_word = Word(word).spellcheck()[0][0]
            if new_word != word:
                new_str_list_for_df.append(new_word)
            else:
                new_str_list_for_df.append('')
        new_str_list.append(word)
    return treebank.detokenize(new_str_list), DataFrame({
        'Original': original_str_list,
        'Corrected': new_str_list_for_df
    })


# def spell_word_pytesseract(word, spell):
#     misspelled = spell.unknown([word])
#     if misspelled == set():
#         return False,''
#     else:
#         for misspell in misspelled:
#             # Get the one `most likely` answer
#             return True, spell.correction(misspell)
#
#     if spelling_checker_var:
#         if 'autocorrect' in checker_package:
#             spell = Speller(lang='en')
#         else:
#             spell = SpellChecker()
#         all_words = [[token, sentence_number + 1, article_number + 1, sentence, article[1],IO_csv_util.dressFilenameForCSVHyperlink(article[2]) ,''] for article_number, article in
#                      enumerate(articles)
#                      for sentence_number, sentence in enumerate(article[0])
#                      for token in NLP.word_tokenize(sentence)]
#
#         list_to_check = all_words
#         word_list = [elmt[0] for elmt in list_to_check]
#         word_freq_dict = {i: word_list.count(i) for i in set(word_list)}
#         for word in list_to_check:
#             word.insert(1, word_freq_dict.get(word[0]))
#             if 'pyspellchecker' in checker_package:
#                 value_tuple = spell_word_pytesseract(word[0],spell)
#             else:
#                 return
#             if value_tuple[0]:
#                 word.append(value_tuple[1])  # returned similar word from check_edit_list
#                 word.append(word_freq_dict.get(value_tuple[1],0))  # returned similar word frequency from check_edit_list
#                 word.append('Typo?')
#             else:
#                 word.append('')
#                 word.append('')
#                 word.append('')
#             print(word)

# the main checking function, takes input:
#   CoreNLPDirectory, inputDir, output_file_path
# now checking for NE list ['CITY', 'LOCATION', 'PERSON']
# output csv header list: ['NNPs', 'sentenceID', 'DocumentID', 'fileName', 'NamedEntity', 'potential_Typo']

# using Levenshtein distance to check for typos
def check_for_typo(CoreNLPDir, inputDir, outputDir, openOutputFiles, createExcelCharts, NERs, similarity_value, checker_package, by_all_tokens_var):
    filesToOpen=[]
    all_word_dict = []
    ner_dict = {}
    if by_all_tokens_var:
        pass
    else:
        if NERs[0] == '*':
            NERs = ['CITY', 'LOCATION', 'PERSON', 'COUNTRY', 'STATE_OR_PROVINCE', 'ORGANIZATION']
        else:
            pass
    articles = []
    folderID=0
    fileID=0
    subfolder=[]
    nFiles = nFolders = 0
    NLP = StanfordCoreNLP('http://localhost', port=9000)

    IO_user_interface_util.timed_alert(GUI_util.window, 3000, 'Word similarity start', 'Started running Word similarity at', True)

    for folder, subs, files in os.walk(inputDir):
        folderID+=1
        print("\nProcessing folder "+str(folderID)+"/"+str(nFolders)+": "+os.path.basename(os.path.normpath(folder)))
        for filename in files:
            if not filename.endswith('.txt'):
                continue
            print("  Processing file:",filename)
            fileID=fileID+1
            dir_path = os.path.join(folder, filename)
            with open(dir_path, 'r', encoding='utf-8', errors='ignore') as src:
                text = src.read().replace("\n", " ")
            sentences = tokenize.sent_tokenize(text)
            articles.append([sentences,filename, dir_path])

    # IO_util.timed_alert(GUI_util.window, 5000, 'Word similarity', 'Finished preparing data...\n\nProcessed '+str(folderID)+' subfolders and '+str(fileID)+' files.\n\nNow running Stanford CoreNLP to get NER values on every file processed... PLEASE, be patient. This may take a while...')
    print('Finished preparing data for folder '+str(folderID)+' subfolders and '+str(fileID)+' files.')

    if by_all_tokens_var:
        all_words = [[token, sentence_number + 1, article_number + 1,sentence, article[1],IO_csv_util.dressFilenameForCSVHyperlink(article[2]), ''] for article_number, article in
               enumerate(articles)
               for sentence_number, sentence in enumerate(article[0])
               for token in NLP.word_tokenize(sentence)]
        temp = [elmt[0] for elmt in all_words]
        all_word_dict = [(item, count) for item, count in collections.Counter(temp).items() if count > 1]
        list_to_check = all_words

    else:
        NER = [[ners[0], sentence_number + 1, article_number + 1, sentence, article[1],IO_csv_util.dressFilenameForCSVHyperlink(article[2]), ners[1]] for article_number, article in
               enumerate(articles)
               for sentence_number, sentence in enumerate(article[0])
               for ners in NLP.ner(sentence) if ners[1] in NERs]
        ner_dict = {}
        for each_ner in NERs:
            temp = [elmt[0] for elmt in NER if elmt[-1] == each_ner]
            ner_dict[each_ner] = [(item, count) for item, count in collections.Counter(temp).items() if count > 1]
        list_to_check = NER

    word_list = [elmt[0] for elmt in list_to_check]
    word_freq_dict = {i: word_list.count(i) for i in set(word_list)}
    # for each element in list_to_check, it is in this format:
    # word, sentenceID, documentID, fileName, NamedEntity

    print('Finished running CoreNLP. Processed '+str(len(list_to_check))+' words. Now computing word differences...')
    # IO_util.timed_alert(GUI_util.window, 5000, 'Word similarity', 'Finished running Stanford CoreNLP...\n\nProcessed '+str(len(list_to_check))+' words.\n\nNow computing word differences... PLEASE, be patient. This may take a while...')
    if by_all_tokens_var:
        for word in list_to_check:
            word.insert(1, word_freq_dict.get(word[0]))
            checker_against = all_word_dict
            value_tuple = check_edit_dist(word[0], checker_against, similarity_value)
            if value_tuple[0]:
                word.append(value_tuple[1])  # returned similar word from check_edit_list
                word.append(value_tuple[2])  # returned similar word frequency from check_edit_list
                word.append('Typo?')
            else:
                word.append('')
                word.append('')
                word.append('')
            print(word)
        else:
            for word in list_to_check:
                word.insert(1, word_freq_dict.get(word[0]))
                # [('word', Count:int)]
                for each_ner in NERs:
                    if word[-1] == each_ner:
                        checker_against = ner_dict.get(each_ner)
                        value_tuple = check_edit_dist(word[0], checker_against, similarity_value)
                        if value_tuple[0]:
                            word.append(value_tuple[1])  # returned similar word from check_edit_list
                            word.append(value_tuple[2])  # returned similar word frequency from check_edit_list
                            word.append('Typo?')
                        else:
                            word.append('')
                            word.append('')
                            word.append('')

        df = pd.DataFrame(list_to_check,
                     columns=['Words', 'Word_frequency_IN_document', 'SentenceID', 'DocumentID','Sentence', 'File_Name','Filename_Path','Named_Entity(NER)',
                              'Similar_word_in_dir', 'Similar_word_frequency_in_dir','Typo?'])
        for index, row in df.iterrows():
            if row['Similar_word_in_dir'] != None:
                tmp = df[df['Words'] == row['Similar_word_in_dir']]
                df.loc[index, 'Number_of_documents_processed'] = tmp.File_Name.nunique()
        # df['Processed_dir'] = inputDir
        df['Processed_dir'] = IO_csv_util.dressFilenameForCSVHyperlink(inputDir)
        df_complete = df[['Words', 'Named_Entity(NER)', 'Word_frequency_IN_document', 'Similar_word_in_dir', 'Similar_word_frequency_in_dir', 'Typo?',
                 'Number_of_documents_processed','SentenceID', 'DocumentID','Sentence','File_Name','Filename_Path','Processed_dir']]
        df_simple = df.drop_duplicates(subset=['Words', 'DocumentID'], keep='last')

        if by_all_tokens_var:
            outputFileName_complete = IO_files_util.generate_output_file_name(inputDir, outputDir, '.csv', 'WordSimil',
                                                                              str(similarity_value), 'Edit_dist_algo',
                                                                              'all_words', 'Full-table')
            outputFileName_simple = IO_files_util.generate_output_file_name(inputDir, outputDir, '.csv', 'WordSimil',
                                                                            str(similarity_value), 'Edit_dist_algo',
                                                                            'all_words', 'Concise-table')
        else:
            outputFileName_complete = IO_files_util.generate_output_file_name(inputDir, outputDir, '.csv', 'WordSimil',
                                                                              str(similarity_value), 'Edit_dist_algo',
                                                                              'NERs', 'Full-table')
            outputFileName_simple = IO_files_util.generate_output_file_name(inputDir, outputDir, '.csv', 'WordSimil',
                                                                            str(similarity_value), 'Edit_dist_algo', 'NERs',
                                                                                'Concise-table')
        if len(df_simple) > 0 and len(df_complete) > 0:
                df_simple.to_csv(outputFileName_simple, index=False)
                df_complete.to_csv(outputFileName_complete, index=False)
                filesToOpen.append(outputFileName_simple)
                filesToOpen.append(outputFileName_complete)

                filesToOpen.append(outputFileName_simple)
                filesToOpen.append(outputFileName_complete)

                if createExcelCharts:
                    Excel_outputFileName=createChart(outputFileName_simple, outputDir, [[10, 10]], '')

                    if Excel_outputFileName != "":
                        filesToOpen.append(Excel_outputFileName)
        IO_user_interface_util.timed_alert(GUI_util.window, 3000, 'Word similarity end', 'Finished running Word similarity at', True)

    if openOutputFiles == True:
        IO_files_util.OpenOutputFiles(GUI_util.window, openOutputFiles, filesToOpen)
        filesToOpen=[] # empty the list to avoid opening files twice

    return filesToOpen
